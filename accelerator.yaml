accelerator:
  displayName: LLM API Client
  description: A template for creating API-based LLM application using Spring Boot 3
  iconUrl: https://raw.githubusercontent.com/cpage-pivotal/dallecool/main/config/ai.png
  tags:
    - java
    - spring
    - cloud-native-devs
    - openai

  imports:
    - name: tap-workload
  options:

  - name: model
    label: Model Catalog
    description: The model catalog which are suitable for natural language tasks, others specialize in code
    required: true
    defaultValue: "hugginface"
    inputType: select
    choices:
      - value: hugginface
        text: Hugging Face
      - value: meta
        text: Meta 
      - value: deci
        text: Deci 
      - value: openassist
        text: Open Assist 
      - value: bigscience
        text: Big Science 

  - name: llm
    label: LLM
    description: The LLM trained data set which will generate the completion. Some models are suitable for natural language tasks, others specialize in code
    required: true
    defaultValue: "ilama"
    inputType: select
    choices:
      - value: davinci
        text: text-davinci-003
      - value: ilama
        text: open-ilama-7b-v2-open-instruct
      - value: xgen
        text: xgen-7b-8k-base
      - value: bert-private
        text: bert-large-pretrained
      - value: stanford_alpaca  
        text: Stanford Alpaca
      - value: openchat
        text: GPT-NeoXT-Chat-Base-20B

  - name: vectordb
    label: Vector Database
    description: Select a claim to a vector database
    required: true
    defaultValue: "none"
    inputType: select
    choices:
      - value: "none"
        text: "None"
      - value: greenplum
        text: Greenplum
      - value: redis
        text: Redis

  - name: java
    label: Langchain (Java) 
    description: Add boiler plate code for use of Java in this LLM module
    inputType: checkbox
    dataType: boolean
    defaultValue: true

  - name: python
    label: LIamaIndex (Python)
    description: Add boiler plate code for use of Python in this LLM module
    inputType: checkbox
    dataType: boolean
    defaultValue: true

  - name: probabilities
    label: Add probabilities
    description: Add response info which Indicates how likely a token was to be generated. Helps to debug a given generation, or see alternative options for a token.
    required: true
    defaultValue: "off"
    inputType: select
    choices:
      - value: "off"
        text: "Off"
      - value: mostLikely
        text: Most likely
      - value: leastlikely
        text: Least likely
      - value: full
        text: Full spectrum

  - name: advancedParams
    label: Advanced openai parameters 
    description: Use caution modifying defaults! can effect perfomance and costs.
    inputType: checkbox
    display: true
    dataType: boolean
    defaultValue: false
  - name: temp
    label: Temperature [0 to 2]
    description: Controls randomness. Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.
    dependsOn:
      name: advancedParams
    inputType: text
    defaultValue: "1.0"
  - name: topp
    label: Top P [0 to 1]
    description: Controls diversity via nucleus sampling. 0.5 means half of all likelihood-weighted options are considered.
    dependsOn:
      name: advancedParams
    inputType: text
    defaultValue: "0.5"
  - name: freqPenalty
    label: Frequency penalty [0 to 1]
    description: How much to penalize new tokens based on their existing frequency in the text so far. Decreases the model's likellhood to repeat the same line verbatim
    dependsOn:
      name: advancedParams
    inputType: text
    defaultValue: "0.0"
  - name: presPenalty
    label: Presence penalty [0 to 1]
    description: How much to penalize new tokens based on whether they appear in the text so far. Increases the model's likelihood to talk about new topics.
    dependsOn:
      name: advancedParams
    inputType: text
    defaultValue: "0.0"
  - name: bestof
    label: Best of [0 to 1]
    description: Generates multiple completions server-side, and displays only the best. Streaming only works when set to 1. Since it acts as a multiplier on the number of completions, this parameters can eat into your token quota very quickly - use caution!
    dependsOn:
      name: advancedParams
    inputType: text
    defaultValue: "0.0"
    

engine:
  chain:
    - merge:
        - include: [ "**/*" ]
          exclude: [ "config/*.yaml", "Tiltfile", "README.md", "catalog/*.yaml", ".github/workflows/**" ]
        - include: [ "Tiltfile" ]
          chain:
            - type: ReplaceText
              substitutions:
                - text: openai
                  with: "#artifactId.toLowerCase()"

        - include: [ "config/*.yaml" ]
          chain:
            - type: ReplaceText
              substitutions:
                - text: ": openai"
                  with: "': ' + #artifactId.toLowerCase()"
            - merge:
                - type: InvokeFragment
                  reference: tap-workload
                - include: [ "**" ]
              onConflict: UseFirst

        - include: [ "README.md" ]
          chain:
            - type: ReplaceText
              substitutions:
                - text: openai
                  with: "#artifactId"

        - include: [ "catalog/*.yaml" ]
          chain:
            - type: ReplaceText
              substitutions:
                - text: openai
                  with: "#artifactId"

    - type: Provenance
